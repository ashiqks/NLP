# -*- coding: utf-8 -*-
"""Time_Series_NLP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GGcinqBvndK8RmTSSQREoOd8VblnOh-1
"""

from google.colab import drive
drive.mount('/content/drive')

# Import necessary packages
import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
from datetime import datetime
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error
from keras.models import Sequential
from keras.layers import Dense, LSTM

# Make a function to parse the date in right format to the dataframe
def parse_date(x):
  return datetime.strptime(x, '%Y %m %d %H')

# Read the csv file and parse date columns
df = pd.read_csv('https://raw.githubusercontent.com/jbrownlee/Datasets/master/pollution.csv', parse_dates=[['year', 'month', 'day', 'hour']], date_parser=parse_date, index_col=0)

# Drop the old index column
df.drop('No', axis=1, inplace=True)

# Rename the columns
df.columns = ['pollution', 'dew', 'temp', 'press', 'wind_dir', 'wind_spd', 'snow', 'rain']

# Replace NaN with 0
df['pollution'].fillna(0, inplace=True)

# Remove first 24 rows as it contains null values for pollution column
df = df[24:]

df.head()

# Convert the dataframe into numpy array
values = df.values
# Instantiate a labelencoder class and encode the labels in the 5th column to numbers
encoder = LabelEncoder()
values[:, 4] = encoder.fit_transform(values[:, 4])

# Use the minmaxscaler of sklear to normalize the values and convert it back to a dataframe
scaler = MinMaxScaler()
df_scaled = pd.DataFrame(scaler.fit_transform(values))

num_vars = df_scaled.shape[1]
cols, names = [], []
# Shift the copies of columns by pushing forward one time using the shift fucntion  
cols.append(df_scaled.shift(1))

names += ['variable_{}(t-1)'.format(i+1) for i in range(num_vars)]

# Shift the copies of columns by pushing backwards one time using the shift fucntion  
cols.append(df_scaled.shift(-1))
names += ['variable_{}(t)'.format(i+1) for i in range(num_vars)]

# Concate the values in the list together to create a new dataframe
df_new = pd.concat(cols, axis=1)

df_new.columns = names

# Drop the NaN values
df_new.dropna(inplace=True)

# Drop all the forward columns except the column for pollution
df_new.drop(df_new.columns[[i for i in range(9, 16)]], axis=1, inplace=True)

# Split the dataset into training and testing
values = df_new.values
num_train_hours = 365 * 24 * 4
train = values[: num_train_hours, :]
test = values[num_train_hours:, :]
train_x, train_y = train[:, :-1], train[:, -1]
test_x, test_y = test[:, :-1], test[:, -1]

train_x = train_x.reshape((train_x.shape[0], 1, train_x.shape[1]))
test_x = test_x.reshape((test_x.shape[0], 1, test_x.shape[1]))
print(train_x.shape, test_x.shape)

print(train_y.shape, test_y.shape)

# Create an sequential model class
model = Sequential()
# Add the LSTM layer with 100 units
model.add(LSTM(100, input_shape=(train_x.shape[1], train_x.shape[2])))
# Add the output dense layer
model.add(Dense(1))
# Compile the model with adam optimizer
model.compile(loss='mse', optimizer='adam')

# Run the model
history = model.fit(train_x, train_y, epochs=50, batch_size=128, validation_data=(test_x, test_y), verbose=2, shuffle=False)
# Plot history
plt.plot(history.history['loss'], label='train')
plt.plot(history.history['val_loss'], label='test')
plt.legend()
plt.show

# Make the predictions with test set
y_pred = model.predict(test_x)
test_x = test_x.reshape(test_x.shape[0], test_x.shape[2])
inv_y_pred = np.concatenate((y_pred, test_x[:, 1:]), axis=1)
# Scale inversely to get the real output
inv_y_pred = scaler.inverse_transform(inv_y_pred)[:, 0]

test_y = test_y.reshape((len(test_y), 1))

inv_y = np.concatenate((test_y, test_x[:, 1:]), axis=1)
# Rescale the test output 
inv_y = scaler.inverse_transform(inv_y)[:, 0]

# Calculate the root mean squared error
rmse = np.sqrt(mean_squared_error(inv_y, inv_y_pred))
print(rmse)

