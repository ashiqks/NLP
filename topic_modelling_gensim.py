# -*- coding: utf-8 -*-
"""topic_modelling_gensim.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qnmjz5TsKgEMKoFEOAUvVGGrgkczWo9V
"""

#import necessary packages
import numpy as np
from sklearn.datasets import fetch_20newsgroups # import the dataset
import spacy # import spacy for nlp preprocessing
from gensim import corpora, models # import classes for creating bag of words & tf-idf
import re # import regex module
import pandas as pd 
from pprint import pprint
import pyLDAvis # import this package for plotting topic model
import pyLDAvis.gensim # import this class to plot based on gensim
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings("ignore",category=DeprecationWarning) # turn off warnings

dataset = fetch_20newsgroups(subset='train', shuffle=True) # load the train data including the labels

nlp = spacy.load('en_core_web_sm') # load the spacy 'en_core_web_sm' model
data = dataset.data # extract only the data
data = [re.sub('\S*@\S*\s?', '', sent) for sent in data] # eliminate e-mail strings from the dataset using the sub method of regex module

texts = [] # create an empty list to store the preprocessed data
allowed_pos = ['ADJ', 'ADV', 'NOUN', 'PROPN', 'VERB'] # only the words having the parts of speech will be included in the dataset
for document in data: # looping over the documents one by one
    text = []
    doc = nlp(document) # return a spacy document object for processing
    for w in doc: # looping over single tokens of the document object
        if  (not w.is_stop) and (not w.is_punct) and (not w.like_num) and (w.pos_ in allowed_pos): # eliminating stop words, punctuations, numbers and the words whose POS is not included on the list described above using features provided by spacy
                text.append(w.lemma_) # take only the lemma of the word
    texts.append(text) # append the documents one by one to the 'texts' list created earlier

bigram = models.Phrases(texts, min_count=1, threshold=1) # create a bigram model class using the Phrases class of gensim package and fit it to the dataset
texts = [bigram[data] for data in texts] # transform and create the actual bigrams off the text
print(texts)

dictionary = corpora.Dictionary(texts) # create a bag of words model class using the Dictionary class of gensim
corpus = [ dictionary.doc2bow(text) for text in texts] # create the bag of words corpus off the Dictionary class
print(corpus)

lda = models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=20, passes=1000) # run the LdaModel class from gensim for topic modelling with our bag of words corpus, the dictionary object, the topic parameter and the number of runs the model has to run to finish modelling

pprint(lda.print_topics()) # print the topics returned by the ldamodel model

print(lda.log_perplexity(corpus)) # compute and print the log perplexity of the lda model

pyLDAvis.enable_notebook() # enable pyLDAvis notebook 
py = pyLDAvis.gensim.prepare(lda, corpus, dictionary) # visualize the topic models

py