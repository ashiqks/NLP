# -*- coding: utf-8 -*-
"""Sentiment_Classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OF9mmnJFIon8KTZxomHER2XHberv_JM-
"""

from google.colab import drive
drive.mount('/content/drive')

# Import necessary modules and packages 
import zipfile, pandas as pd, requests, string, os
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from keras.models import Sequential
from keras.layers import GRU, LSTM, Dense, Embedding, SpatialDropout1D, Dropout
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
import numpy as np

# Download the dataset
d = requests.get('https://archive.ics.uci.edu/ml/machine-learning-databases/00331/sentiment%20labelled%20sentences.zip')
# Save the zip file 
with open('datas.zip', 'wb') as dd:
  dd.write(d.content)
# Unzip and extract the files
with zipfile.ZipFile("datas.zip","r") as zip_d:
    zip_d.extractall()

os.rename('sentiment labelled sentences', 'datas')

# Import the three data text files and rename the columns as follows
df_amazon = pd.read_csv('datas/amazon_cells_labelled.txt', delimiter='\t', encoding='utf-8')
df_amazon.columns = ['Review', 'Mark']

df_imdb = pd.read_csv('datas/imdb_labelled.txt', delimiter='\t', encoding='utf-8')
df_imdb.columns = ['Review', 'Mark']

df_yelp = pd.read_csv('datas/yelp_labelled.txt', delimiter='\t', encoding='utf-8')
df_yelp.columns = ['Review', 'Mark']

# Concatenate the three dataframes into one by rows
df = pd.concat([df_amazon, df_imdb, df_yelp], axis=0)
print(df.head())
print(len(df))

# Create an empty list and convert the values in the review column to a list
review_list = []
lines = df['Review'].values.tolist()

# For each line the list
for l in lines:
  # Tokenize the line
  tokens = word_tokenize(l)
  # Convert every word into lower
  tokens = [w.lower() for w in tokens]
  # Create the punctuation table
  table = str.maketrans('', '', string.punctuation)
  # Apply the punctuation table to correct the punctuations
  stripped = [w.translate(table) for w in tokens]
  # Check if all words are only alphabetic
  words = [word for word in stripped if word.isalpha()]
  # Get the stop words
  stop_words = set(stopwords.words('english'))
  # Remove the stop lists
  words = [w for w in words if w not in stop_words]
  # Append the words to the list created earlier 
  review_list.append(words)
  
# Get the maximum length inside the list 
max_length = max(len(l) for l in review_list)

# Instantiate a tokenizer object
tokenizer = Tokenizer()
# Fit it on the review list words
tokenizer.fit_on_texts(review_list)
# Get the word indices 
word_index = tokenizer.word_index
# Add one to the length of word index as the index starts from 0
vocab_size = len(word_index) + 1
# Convert the text into sequences
sequences = tokenizer.texts_to_sequences(review_list)
# Pad the sequences to a fixed length
words = pad_sequences(sequences, maxlen=max_length)
# Get the mark column as the target variable
sentiment = df['Mark'].values
print(words.shape)
print(sentiment.shape)

# Generate the data set for the model
validation_split = 0.2
indices = np.arange(words.shape[0])
np.random.shuffle(indices)
words = words[indices]
sentiment = sentiment[indices]
num_validation_samples = int(validation_split * words.shape[0])
print(len(words))
x_train = words[: -num_validation_samples]
y_train = sentiment[: -num_validation_samples]
x_test = words[-num_validation_samples: ]
y_test = sentiment[-num_validation_samples: ]

# Create an embedding matrix from the downloaded glove vector file
def create_embedding_matrix(filepath, word_index, embedding_dim):
    # Adding  1 because of reserved 0 index
  vocab_size = len(word_index) + 1  
  # Create a matrix initialized with zeros to hold the glove vector
  embedding_matrix = np.zeros((vocab_size, embedding_dim))

  with open(filepath) as f:
    for line in f:
      # For each line get the word and the rest are the glove vector values     
      word, *vector = line.split()
      # Get the vector only for the words in the dataset
      if word in word_index:
        idx = word_index[word] 
        embedding_matrix[idx] = np.array(
                  vector, dtype=np.float32)[:embedding_dim]

  return embedding_matrix

embedding_dim = 50
# Download the glove vectors, save it, unzip all 
z = requests.get('http://nlp.stanford.edu/data/glove.6B.zip')
with open('glovev.zip', 'wb') as zz:
  zz.write(z.content)

with zipfile.ZipFile("glovev.zip","r") as zip_z:
    zip_z.extractall()

# Create the 
embedding_matrix = create_embedding_matrix('glove.6B.50d.txt', tokenizer.word_index, embedding_dim)

# Create the sequential class model
model = Sequential()
# Add the embedding layer with the downloaded glove vectors
embedding_layer = Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], input_length=max_length, trainable=False)
model.add(embedding_layer)

# Add an LSTM layer
model.add(LSTM(256))
# Add the output layer
model.add(Dense(1, activation='sigmoid'))
# Compile the model with binary crossentropy and adam optimizer
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the model with the dataset
model.fit(x_train, y_train, batch_size=128, epochs=5)

# Evaluate the model with test dataset
score,acc = model.evaluate(x_test, y_test, verbose = 2)
print("score: %.2f" % (score))
print("acc: %.2f" % (acc))

test = ['Not worth it']
#vectorizing the test by the pre-fitted tokenizer instance
test = tokenizer.texts_to_sequences(test)
#padding the test data
test = pad_sequences(test, maxlen=max_length)
print(test)
# Make the prediction
sentiment = model.predict(test,batch_size=1,verbose = 2)[0]
if(np.argmax(sentiment) == 0):
    print("negative")
elif (np.argmax(sentiment) == 1):
    print("positive")