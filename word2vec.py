# -*- coding: utf-8 -*-
"""word2vec.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1n1UY1-W5KErQjd94D_cVFG-fB8nPULNt
"""

# import necessart modules and packages
from gensim.models import Word2Vec
import spacy
from sklearn.decomposition import PCA
from matplotlib import pyplot as plt

# import the spacy english nlp model
nlp = spacy.load("en_core_web_sm")
# convert the sentences into doc object
doc = nlp(u"This is a sentence. This is another sentence.")
sentences = []
# use the spacy built-in sentencizer to split each sentences
for sent in doc.sents:
    sentences.append(sent.text)
    
print(sentences)

# function to tokenize the words from the list
def tokenizer(sent):
  sentence_list = []
  for s in sent:
    tokens = []
    doc = nlp(s)
    for token in doc:
      if not token.is_punct: # to not include punctuations
        tokens.append(token.text)
    sentence_list.append(tokens)
  return sentence_list
  
sent_list = tokenizer(sentences)
  
print(sent_list)

# generating the word2vec using the Word2Vec model from gensim
model = Word2Vec(sentence_list, min_count=1)
print(model)

words = list(model.wv.vocab)
print(words)

# print the embedding of the word 'sentence'
print(model['sentence'])

# save the model
model.save('word2vec.bin')

# extract out all the word for applying the PCA transformation inorder to visualize the embedding
X = model[model.wv.vocab]

# initializing pca object and apply fit_transform with two principle components
pca = PCA(n_components=2)
x_pca = pca.fit_transform(X)

# plot the graph
plt.scatter(x_pca[:, 0], x_pca[:, 1])
for i, word in enumerate(words):
  plt.annotate(word, xy=(x_pca[i, 0], x_pca[i, 1]))
plt.show()

