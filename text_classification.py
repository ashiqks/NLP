# -*- coding: utf-8 -*-
"""Text_Classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jen0O6HnshFGExnPT_OZ6p9CjJNGsniR
"""

# Import necessary modules and packages 
from sklearn.preprocessing import MultiLabelBinarizer
import zipfile, pandas as pd, requests, string, os
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from keras.models import Sequential
from keras.layers import GRU, LSTM, Dense, Embedding, SpatialDropout1D, Dropout, Conv1D, GlobalMaxPool1D
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
import numpy as np
import spacy

df = pd.read_csv('https://raw.githubusercontent.com/vietnguyen91/Very-deep-cnn-tensorflow/master/data/test.csv')
df.columns = ['class', 'text_1', 'text_2']
df['text'] = df['text_1'] + ' ' + df['text_2']
del df['text_1']
del df['text_2']
df.head()

# Create an empty list and convert the values in the review column to a list
review_list = []
lines = df['text'].values.tolist()
nlp = spacy.load('en_core_web_sm')
# For each line the list
for l in lines:
  # Tokenize the line
  tokens = word_tokenize(l)
  # Convert every word into lower
  tokens = [w.lower() for w in tokens]
  # Create the punctuation table
  table = str.maketrans('', '', string.punctuation)
  # Apply the punctuation table to correct the punctuations
  stripped = [w.translate(table) for w in tokens]
  # Check if all words are only alphabetic
  words = [word for word in stripped if word.isalpha()]
  # Get the stop words
  stop_words = set(stopwords.words('english'))
  # Remove the stop lists
  words = [w for w in words if w not in stop_words]
  # Create spacy language model
  nlp_doc = ' '.join(words)
  # Make a doc object
  doc = nlp(nlp_doc)
  # Lemmatize the words
  words = [d.lemma_ for d in doc] 
  # Append the words to the list created earlier 
  review_list.append(words)
  
# Get the maximum length inside the list 
max_length = max(len(l) for l in review_list)

review_list

1
# Instantiate a tokenizer object
tokenizer = Tokenizer()
# Fit it on the review list words
tokenizer.fit_on_texts(review_list)
# Get the word indices 
word_index = tokenizer.word_index
# Add one to the length of word index as the index starts from 0
vocab_size = len(word_index) + 1
# Convert the text into sequences
sequences = tokenizer.texts_to_sequences(review_list)
# Pad the sequences to a fixed length
words = pad_sequences(sequences, maxlen=max_length)
# Get the mark column as the target variable
sentiment = df['class'].values
print(words.shape)
print(sentiment.shape)

# Generate the data set for the model
validation_split = 0.2
indices = np.arange(words.shape[0])
np.random.shuffle(indices)
words = words[indices]
sentiment = sentiment[indices]
num_validation_samples = int(validation_split * words.shape[0])
print(len(words))
x_train = words[: -num_validation_samples]
y_train = sentiment[: -num_validation_samples]
x_test = words[-num_validation_samples: ]
y_test = sentiment[-num_validation_samples: ]

# Create an embedding matrix from the downloaded glove vector file
def create_embedding_matrix(filepath, word_index, embedding_dim):
    # Adding  1 because of reserved 0 index
  vocab_size = len(word_index) + 1  
  # Create a matrix initialized with zeros to hold the glove vector
  embedding_matrix = np.zeros((vocab_size, embedding_dim))

  with open(filepath) as f:
    for line in f:
      # For each line get the word and the rest are the glove vector values     
      word, *vector = line.split()
      # Get the vector only for the words in the dataset
      if word in word_index:
        idx = word_index[word] 
        embedding_matrix[idx] = np.array(
                  vector, dtype=np.float32)[:embedding_dim]

  return embedding_matrix

embedding_dim = 50
# Download the glove vectors, save it, unzip all 
z = requests.get('http://nlp.stanford.edu/data/glove.6B.zip')
with open('glovev.zip', 'wb') as zz:
  zz.write(z.content)

with zipfile.ZipFile("glovev.zip","r") as zip_z:
    zip_z.extractall()

# Create the 
embedding_matrix = create_embedding_matrix('glove.6B.50d.txt', tokenizer.word_index, embedding_dim)

# Create the sequential class model
model = Sequential()
# Add the embedding layer with the downloaded glove vectors
embedding_layer = Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], input_length=max_length, trainable=False)
model.add(embedding_layer)

# Add a spatial dropout layer
model.add(SpatialDropout1D(0.2))
# Add the 1D convolutional layer with relu activation
model.add(Conv1D(256, 3, activation='relu'))
# Max pool the convolutional layer
model.add(GlobalMaxPool1D())
# Add a dense layer with 256 neurons
model.add(Dense(256, activation='relu'))
# Add the output layer
model.add(Dense(1, activation='sigmoid'))
# Compile the model with binary crossentropy and adam optimizer
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the model with the dataset
model.fit(x_train, y_train, batch_size=128, epochs=2)

# Evaluate the model with test dataset
score,acc = model.evaluate(x_test, y_test, verbose = 2)
print("score: %.2f" % (score))
print("acc: %.2f" % (acc))