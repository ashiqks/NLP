# -*- coding: utf-8 -*-
"""Bert_Next_Sentence_Predictor.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_UVmsZ5hBQUbbdMdyrEJmrhE_7ClaH89
"""

'''
Tutorial to make use of bert's pretrained BertForNextSentencePrediction to
predict if the next sentence of a given sentence is based on it or not.
Used the NYTimes archived RSS feed data on technology, inputting the title as the 
first sentence and predicting if the corresponding description of the title is
based on the title or not.
'''


# Import necessary packages
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim import lr_scheduler
import matplotlib.pyplot as plt
import os
import copy
from torch.utils.data import TensorDataset, DataLoader, SequentialSampler 
import torch.nn.functional as F
from pytorch_pretrained_bert import BertTokenizer, BertForNextSentencePrediction, BertConfig
import requests 
import pandas as pd
import regex as re
from sklearn.model_selection import train_test_split
import time
from fastprogress import progress_bar
from xmljson import BadgerFish 
from xml.etree.ElementTree import fromstring


# Use the HTTP GET method of requests library to get the xml dataset and decode it to string format
r = requests.get('https://rss.nytimes.com/services/xml/rss/nyt/Technology.xml').content.decode()

# Initialize the badgerfish class of xmljson package to convert xml data to dictionary format
bf = BadgerFish(dict_type=dict)
# Convert the xml data to python dictionary format
xml_dict = dict(bf.data(fromstring(r)))


title_list = []
# Get the title data from the rss feed and append to a list 
for title in xml_dict['rss']['channel']['item']:
    title_list.append(title['title']['$'])
print(title_list[:10])

print(len(title_list))

desc_list = []
# Get the descriptio data corresponding to the title and append to another list
for desc in xml_dict['rss']['channel']['item']:
    desc_list.append(desc['description']['$'])
print(desc_list[:10])
print(len(desc_list))

# Create a pandas dataframe with the title and description list 
data_tuples = list(zip(title_list, desc_list))
df = pd.DataFrame(data_tuples, columns=['title', 'description'])
print(df.head())


# Create a class to hold the input tensors and assign the values
class Input_Features(object):
  def __init__(self, input_ids, token_type_ids, input_mask, target):
    self.input_ids = input_ids
    self.token_type_ids = token_type_ids
    self.input_mask = input_mask
    self.target = target
    
    

# Method to truncate the each  pair of data to a given maximum sequence length
def _truncate_seq_pair(token_a, token_b, max_seq_len):
  while True:
    total_length = len(token_a) + len(token_b)
    # If the combined length of the  each pair is lesser than that of the given maximum sequence then exit the function, else remove extra strings from the dataset
    if total_length < max_seq_len:
      break
    if len(token_a) > len(token_b):
      token_a.pop()
    else:
      token_b.pop()
      
      
# Create a bert tokenizer object from BertTokenizer class
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)

# Function to take the inputs and convert to features 
def convert_seq_to_features(titles, descs, max_seq_len):
  features = []
  # Load the the pair of dataset by zipping the title and description lists
  for title, desc in zip(titles, descs):
    # Tokenize the title and description using the tokenize method of the bert tokenizer class
    token_a = tokenizer.tokenize(title)
    token_b = None
    token_b = tokenizer.tokenize(desc)
    
    # Truncate the pair by using the _truncate_seq_pair function , here -3 implies making space for special characters that the bert model needed
    _truncate_seq_pair(token_a, token_b, max_seq_len-3)
    
    # Preppend the beginning of tokenized list with [['CLS']] and append with '[['SEP']]' special tokens
    tokens = ['[CLS]'] + token_a + ['[SEP]']
    # Create a token type id list with length of the above created tokens list, populated all by zeros, actually to separate between the two input sentences
    token_type_ids = [0] * len(tokens)
    
    if token_b:
      # Append the description tokens to the tokens list and finally append the [['SEP']] special token
      tokens += token_b + ['[SEP]']
      # Extend the token type id list with a length of length of token_b + 1 for the [['SEP']] token 
      token_type_ids += [1] * (len(token_b) + 1)
    
    # Convert the tokens to the corresponding token ids of the bert tokenizer class
    input_ids = tokenizer.convert_tokens_to_ids(tokens)
    # Make a input mask list with the length of the input token ids populated by 1, so as to differentiate between the actual tokens and the following padding
    input_mask = [1] * len(input_ids)
    
    # Create a padding list with all zeros where its length the difference between the maximum sequence length and input ids list length  
    padding = [0] * (max_seq_len - len(input_ids))
    # Pad the following lists to make the length of the input sets uniform
    input_ids += padding
    token_type_ids += padding
    input_mask += padding
    
    # Load the lists to the Input_Feature class and append it to a list
    features.append(Input_Features(input_ids, token_type_ids, input_mask, target=1))
  return features

# Create the input pairs using the convert_seq_to_features with the title list as the first argument followed by the description list and the maximum sequence length 
matching_pairs = convert_seq_to_features(df['title'].tolist(), df['description'].tolist(), 256)

# Find the device on which the model will be operated on, either gpu or cpu
device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu:0')
print(device)

# Instantiate the BertForNextSentencePrediction model with the weights of 'bert-base-uncased' and add the 'to' method with the argument device 
model = BertForNextSentencePrediction.from_pretrained('bert-base-uncased').to(device)

print(model)

# Convert the input lists into pytorch tensors
all_input_ids = torch.tensor([f.input_ids for f in matching_pairs], dtype=torch.long)
all_token_type_ids = torch.tensor([f.token_type_ids for f in matching_pairs], dtype=torch.long)
all_input_mask = torch.tensor([f.input_mask for f in matching_pairs], dtype=torch.long)


BATCH_SIZE = 128
# Create a dataset using the TensorDataset class from pytorch with all the input list
test_data = TensorDataset(all_input_ids, all_token_type_ids, all_input_mask)
# Create a sequential sampler class for sampling the dataset sequentially
test_sampler = SequentialSampler(test_data)
# Instantiate a DataLoader class from pytorch with the TensoDataset object, followed by the sampler object and the batch size
dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=BATCH_SIZE)

# Turn on the evaluation mode of the model
model.eval()

result = []
# Load the inputs by batches from the DataLoader object with the dataloader object is wrapped around by the progress_bar class  of fastprogress package to make a progress bar
for input_ids, token_type_ids, input_mask in progress_bar(dataloader):
  # Take the inputs and add the device information
  input_ids = input_ids.to(device)
  token_type_ids = token_type_ids.to(device)
  input_mask = input_mask.to(device)
  
  # Calculating the result with the gradient calculation is halted
  with torch.no_grad():
    # Give the inputs to the model, softmax the output, take only the first column as it is only 2 columned, taking one column will give us the desired result
    res = F.softmax(model(input_ids, token_type_ids, input_mask ), dim=1)[:, 0]
    # Convert the output into numpy and append to the list
    result.append(res.detach().cpu().numpy())
    
#print(total_time)
result = np.concatenate(result)

print(result)

print(sum(result > 0.99), sum(result > 0.9), sum(result < 0.5))