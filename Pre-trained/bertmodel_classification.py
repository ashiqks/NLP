# -*- coding: utf-8 -*-
"""BertModel_Classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uTvueaEyCMDvu4Yd-MNe7M20kQ8hSuCN
"""

'''

Determing the classification of sentiments using
the pretrained Bert model using Pytorch deep learning library

'''



# Import necessary packages
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim import lr_scheduler
import matplotlib.pyplot as plt
import os
import copy
from torch.utils.data import Dataset, DataLoader
import torch.nn.functional as F
from pytorch_pretrained_bert import BertTokenizer, BertModel, BertConfig
import requests 
import pandas as pd
import regex as re
from sklearn.model_selection import train_test_split
import time
from fastprogress import progress_bar



# Find the device information
device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu:0')
print(device)

# Create a class inheriting from nn.Module of pytorch to fine tune the bert model
class Bert_Classification(nn.Module):
  def __init__(self, num_labels=2):
    #Initialize parent class 
    super().__init__()
    # Assign the number of classes
    self.num_labels = num_labels
    # Create a BertModel with the weigths 'bert-base-uncased' and pass in the device information
    self.bert = BertModel.from_pretrained('bert-base-uncased').to(device)
    # Create a dropout layer with the parameter defined in the configuration of the BertConfig class 
    self.dropout = nn.Dropout(config.hidden_dropout_prob).to(device)
    # Create a linear layer with parameters from the config class and the number of classes
    self.classifier = nn.Linear(config.hidden_size, self.num_labels).to(device)
    # Initialize the weight of the linear classifier layer with xavier normal values
    nn.init.xavier_normal_(self.classifier.weight).to(device)
    
  # Define the forward function takinng in the necessary arguments
  def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):
    # Get the output from the bert model with the corresponding inputs
    _, output = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=None)
    # Add the dropout layer to the bert layers
    output = self.dropout(output)
    # Add the linear classifier layer to the dropout  to get the outputs
    logits = self.classifier(output)
    return logits
  
  
# Create the configuration from the BertConfig class
config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768, num_hidden_layers=12, num_attention_heads=12, intermediate_size=3702)
num_labels = 2
# Instantiate the custom bert model
model = Bert_Classification(num_labels)

# Create a tokenzier object from the BertTokenizer with the weights 'bert-base-uncased'
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)

r = requests.get('https://raw.githubusercontent.com/sugi-chan/custom_bert_pipeline/master/IMDB%20Dataset.csv').content.decode('utf-8')
with open('imdb_review.csv', 'w') as write_file:
  write_file.write(r)

# Read the csv file and make pandas dataframe from it
df = pd.read_csv('imdb_review.csv')
print(df.head())

# A small preprocessing step to eliminate the unawamted <br /><br /> characters
df['review'] = df['review'].replace(to_replace=r'<br /><br />', value=" ", regex=True)


# Make the review column as the input with the sentiment column as the target variable
x = df['review']
y = df['sentiment']
# Split the dataset to training and test datasets and convert them to list objects
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)
x_train = x_train.values.tolist()
x_test = x_test.values.tolist()

# One-hot encode the target variable using pandas get_dummies method
y_train = pd.get_dummies(y_train).values.tolist()
y_test = pd.get_dummies(y_test).values.tolist()


max_seq_len = 256

# Define the dataset class inheriting from the Dataset class of pytorch to efficiently to iterate the inputs
class Text_Dataset(Dataset):
  
  def __init__(self, input_list, transform=None):
    self.input_list = input_list
    self.transform = transform
  
  # Create a __getitem__ magic method to tokenize the independent variable using the tokenize method of bert tokenizer
  def __getitem__(self, index):
    tokenized_text = tokenizer.tokenize(self.input_list[0][index])
    # Trim if the length of the tokens is greater than a predefined length 
    if len(tokenized_text) > max_seq_len:
      tokenized_text = tokenized_text[:max_seq_len]
    
    # Convert the tokens to the corresponding token ids of bert tokenizer
    token_ids = tokenizer.convert_tokens_to_ids(tokenized_text)
    attention_mask = [1] * len(token_ids)
    # Padd the above the list to a fixed length if the length is smaller than that of the predefined length
    padding = [0] * (max_seq_len - len(token_ids))
    token_ids += padding
    attention_mask += padding
    # Convert to torch tensors
    token_ids = torch.tensor(token_ids)
    attention_mask = torch.tensor(attention_mask)
    # Get the target variable and convert to torch tensors
    sentiment = self.input_list[1][index]
    label_list = [torch.from_numpy(np.array(sentiment))][0]
    return token_ids, attention_mask, label_list
   
  # Create the __len__ magic method to return the length of the dataset  
  def __len__(self):
    return len(self.input_list[0])
  
  
batch_size = 32
train_list = [x_train, y_train]
test_list = [x_test, y_test]

# Create the train and test dataset using the Text_Dataset class
train_data = Text_Dataset(train_list)
test_data = Text_Dataset(test_list)


# Load the dataset to the DataLoader class for both the train and test phases
dataloader_dict = {'train': DataLoader(train_data, batch_size=5, shuffle=False, num_workers=0),
                   'test': DataLoader(test_data, batch_size=1, shuffle=True, num_workers=0)}


dataset_size = {'train': len(x_train),
                'test': len(x_test)}


lr_bert = 0.00001
lr_last_layer = 0.001
# Create the adam optimizer from pytorch and apply the learning rates separately to the original bert model followed by the classifier layer
optim_model = optim.Adam([
    
    {'params': model.bert.parameters(), 'lr': lr_bert},
    {'params': model.classifier.parameters(), 'lr': lr_last_layer}
])

# Instantiate the cross entropy loss function from pytorch
criterion = nn.CrossEntropyLoss()

# Instantiate a learning rate scheduler by giving the optimizer object 
exp_lr_decay = lr_scheduler.StepLR(optim_model, step_size=10, gamma=0.1)


# Function to train and evaluate, the parameters are model, loss function, optimizer object, learning rate scheduler and the number of epochs
def training(model, criterion, optimizer, scheduler, num_epochs=100):
  
  print('Starting')
  # Make an independent of the model parameters
  best_mode_dict = copy.deepcopy(model.state_dict())
  # Assign a variable with some value inorder to compare the loss value of the training
  best_loss = 100
  
  # Wrap the progress_bar from fastprogress to view a progress bar
  for i in progress_bar(range(num_epochs)):
    print('Epoch {}/{}'.format(i, num_epochs-1))
    
    # Train and test over the dataset
    for phase in ['train', 'test']:
      # Learning rate scheduler should make changes only in the training phase and put the model in the training mode
      if phase == 'train':
        scheduler.step()
        model.train()
      # If the phase is test, turn on the evaluation mode of the model
      else:
        model.eval()
      
      # Create some variables for measuring the training procedure 
      running_loss = 0.0
      sentiment_corrects = 0.0
      
      # Load the dataset as per the phase
      for inputs, attention_mask, sentiments in progress_bar(dataloader_dict[phase]):
        # Load the device information to the inputs
        inputs = inputs.to(device)
        attention_mask = attention_mask.to(device)
        sentiments = sentiments.to(device)
        # Clear the optimizer values
        optimizer.zero_grad()
        
        # Enable the gradient changes in the training phase
        with torch.set_grad_enabled(phase=='train'):
          # Get the output from the model
          outputs = model(inputs, attention_mask=attention_mask)
          # Softmax the output
          outputs = F.softmax(outputs, dim=1)
          # Make the loss function
          loss = criterion(outputs, torch.max(sentiments.float(), 1)[1])
          
          if phase == 'train':
            # Calculate the loss between the predicted and target variables only in the training phase
            loss.backward()
            # Optimizer should run only in training phase
            optimizer.step()
            
        # Calculate the loss value for each batch of dataset
        running_loss += loss.item() * inputs.size(0)
        # Get the number of correctly predicted values for each batch of dataset
        sentiment_corrects += torch.sum(torch.max(outputs, 1)[1] == torch.max(sentiments, 1)[1])
        
      # Calculate the loss and accuracy for each iteration
      epoch_loss = running_loss / dataset_size[phase]
      print(epoch_loss)
      sentiment_acc = sentiment_corrects.double() / dataset_size[phase]
      print(sentiment_acc)
      
      # In the training phase if the current loss is lesser than values stored in the variable best_loss, change the latter's value to the current loss
      if phase == 'test' and epoch_loss < best_loss:
        best_loss = epoch_loss
        # Get the model's  new parameters
        best_model_dict = copy.deepcopy(model.state_dict())
        # Save the model with the best parameters
        torch.save(model.state_dict(), 'best_model.pth')
        
  # Load the model with the best parameters
  model.load_state_dict(best_model_dict)
  return model

# Train and test on the dataset
trained_model = training(model, criterion, optim_model, exp_lr_decay, 3)